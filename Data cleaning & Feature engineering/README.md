# README – Bat vs Rat: The Forage Files (HIT140 Project)

## 🧩 Project Overview
This repository contains all Python scripts and datasets used to complete **Objective 2 – Investigation A & B** of the HIT140 *Foundations of Data Science* unit.  
The workflow replicates a real-world data science pipeline that processes, checks, merges, and engineers new features from two zoological datasets examining the foraging interactions between **Egyptian Fruit Bats** and **Black Rats**.

The workflow consists of **four major steps**, executed in order.

---

## ⚙️ 1. cleaning.py – Data Cleaning
**Purpose:**  
Transform raw datasets (`dataset1.csv`, `dataset2.csv`) into consistent, analysis-ready files.

**Main actions:**  
- Convert date/time and numeric types  
- Handle missing values ethically (`habit` → “unknown”)  
- Remove duplicate rows  
- Fix invalid time logic (swap errors → set to `NaT`)  
- Create QA helper columns (`rat_present`, `rat_presence_duration`, `arrival_delay_bin`)  
- Flag potential outliers using the IQR method  

**Outputs:**  
- `cleaned_dataset1.csv`  
- `cleaned_dataset2.csv`  
- Optional review logs (`review_time_issues_df1.csv`, `review_time_issues_df2.csv`)

**Run command:**  
```bash
python cleaning.py
```
---

## 🔍 2. checking.py – Data Quality Verification
**Purpose:**  
Validate that the cleaned datasets are reliable and consistent before analysis.

**Main checks:**  
- Missing values and data structure (`.info()` summaries)  
- Duplicate row counts  
- Time-logic sanity (`rat_period_end ≥ rat_period_start`)  
- Outlier flag statistics  
- Sanity preview of engineered QA columns  
- Quick descriptive metrics for risk, latency, and rat activity  

**Outputs:**  
Terminal summary printed to screen (no new CSVs unless you add review exports).  

**Run command:**  
```bash
python checking.py
```

If no issues are reported (duplicates = 0, negative durations = 0, etc.), proceed to the next stage.

---

## 📅 3. merge_cleaned_monthly_v2.py – Monthly Aggregation & Merge
**Purpose:**  
Aggregate both cleaned datasets by **month** and combine them into one summary table for seasonal comparison.

**Main actions:**  
- Compute monthly means (bat behaviour metrics) and monthly sums (rat activity counts)  
- Outer-join on `month` to keep all available months  
- Sort results in calendar order  

**Outputs:**  
- `monthly_merged_v2.csv`  

**Run command:**  
```bash
python merge_cleaned_monthly_v2.py
```

This file forms the bridge between cleaning and feature engineering.

---

## 🧠 4. feature_engineering_pipeline.py – Feature Engineering
**Purpose:**  
Generate new, interpretable features for **Investigation A (risk perception)** and **Investigation B (seasonal change)**.

**Main actions:**  
- Encode seasonality (`month_num`, `season_sin`, `season_cos`)  
- Compute behavioural ratios (`reward_to_risk`, `landing_efficiency`, `landing_per_food`)  
- Derive rat-pressure metrics (`rat_time_ratio`)  
- Calculate time-alignment difference (`sunset_alignment_diff`)  
- Create interaction terms (`reward_x_rat`, `risk_x_rat`)  
- Add rolling 3-month means and z-scores for trend analysis  
- Export final dataset for analysis or modelling  

**Outputs:**  
- `monthly_merged.csv` (intermediate summary)  
- `monthly_features.csv` (final feature matrix ready for analysis)  

**Run command:**  
```bash
python feature_engineering_pipeline.py
```

---

## 🧾 5. (Optional) Feature_Engineering_Report.docx
**Purpose:**  
Explains all methods, rationale, and outcomes (~1500 words, APA 7th style).  
Include this file as the written component of your submission.

---

## ▶️ Recommended Execution Order
Run each step **in sequence** from the project root folder:

1. **Cleaning:**  
   `python cleaning.py`
2. **Checking:**  
   `python checking.py`
3. **Monthly Merge:**  
   `python merge_cleaned_monthly_v2.py`
4. **Feature Engineering:**  
   `python feature_engineering_pipeline.py`

---

## 📁 Final Output Files

**Cleaning stage:**  
- `cleaned_dataset1.csv` – Cleaned and standardised version of dataset 1 (bat landing events).  
- `cleaned_dataset2.csv` – Cleaned and standardised version of dataset 2 (rat activity intervals).  
→ These two files form the reliable base data for all following steps.  

**Checking stage:**  
- Terminal output only.  
→ Confirms that duplicates, missing values, and time logic issues have been properly handled before proceeding.  

**Monthly merge stage:**  
- `monthly_merged_v2.csv` – A single month-level summary that combines key variables from both cleaned datasets.  
→ Used to observe seasonal patterns and prepare for feature creation.  

**Feature engineering stage:**  
- `monthly_merged.csv` – Intermediate summary automatically generated by the feature engineering script.  
- `monthly_features.csv` – The final feature matrix containing all engineered variables (rat pressure, reward-to-risk ratio, seasonality encodings, etc.) ready for analysis and visualisation.  

**Documentation:**  
- `Feature_Engineering_Report.docx` – A 1500-word written report explaining the process, rationale, and outcomes following APA 7th style.

---

## 💡 Notes
- All scripts are written in **Python 3.10+** and use `pandas`, `numpy`, and `calendar` libraries.  
- Avoid manual editing of intermediate CSVs; always regenerate them by rerunning the scripts to ensure reproducibility.  
- Use `python -m pip install pandas numpy` before running if libraries are missing.  
- This pipeline is fully ethical: no data fabrication or deletion of valid observations.